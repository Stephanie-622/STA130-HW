{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b0a99c6",
   "metadata": {},
   "source": [
    "Question 1: \n",
    "\n",
    "1. Simple linear Regression shows the relationship between an outcome variable Y and a single predict X: Y = beta0 + beta1X. However, the Multiple Linear Regression xpands on this by modeling the relationship between Y and two or more than two predicts X: Y = beta0 + beta1X1 + beta2X2\n",
    "\n",
    "2. In Simple Linear Regression, a continuous variable X represents a measurable quantity (e.g., temperature, weight), with a model form: Y = beta0 + beta1X, An indicator variable (also called a binary variable) represents categories (e.g., gender, presence or absence), where X takes values 0 or 1: Y = beta0 + beta11(X) where 1(X) is the indicator function that outputs 1 if the category is present and 0 otherwise. This approach captures group differences by adjusting the outcome by a constant amount beta1 for one group relative to another.\n",
    "\n",
    "3. When an indicator variable Zis introduced alongside a continuous variable X in a Multiple Linear Regression model, the form becomes: Y = beta0 + beta1X + beta21(Z). The model’s behavior adjusts based on the category defined by Z, effectively modeling different intercepts for each group while sharing the same slope for Xacross groups. This form allows the model to reflect variations in the intercept based on Z, which is particularly useful when each category (e.g., male vs. female) affects the outcome differently.\n",
    "\n",
    "4. When an interaction term between a continuous variable X and an indicator variable Z is added, the model becomes:Y = beta0 + beta1X + beta21(Z) + beta3X·1(Z). This interaction allows the slope of X to vary based on Z, making it possible to model different relationships between X and Y for each group. This form is useful in cases where the influence of X changes depending on the category (e.g., the effect of income on spending might vary by urban vs. rural status).\n",
    "\n",
    "5. When modeling with indicator variables from a non-binary categorical variable (e.g., region with values North, South, East, and West), we create \"number of categories minus one\" indicator variables. If “North” is chosen as the baseline, the model with three indicators Zsouth, Zeast and Zwest would be: Y = beta0 + betasouth1(Zsouth) + betaeast1(Zeast) + betawest1(Zwest) The binary encoding of the categories allows the model to capture differences among multiple categories by comparing each to the baseline, facilitating interpretation of group differences relative to the baseline. This form is common in models with categorical variables, such as in predicting customer preferences by region.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6983a",
   "metadata": {},
   "source": [
    "Question 2: \n",
    "\n",
    "Variables and Potential Interactions in the Advertising Scenario\n",
    "\n",
    "In this scenario, the company’s outcome variable is Sales generated from advertising campaigns, which can represent the revenue or the number of products sold based on the advertising spend. The predictor variables are the TV Advertising Budget (TV) and the Online Advertising Budget (Online), both of which are continuous variables representing the amount spent on each type of advertising.\n",
    "\n",
    "There is a potential interaction between the two advertising budgets. The scenario suggests that the effectiveness of TV advertising might be enhanced when the online advertising budget is high, and vice versa. This means that the impact of one advertising channel on sales could depend on the spending level in the other channel, which is a classic case of interaction in regression modeling.\n",
    "\n",
    "\n",
    "\n",
    "Linear Models Without and With Interaction (Continuous Predictors)\n",
    "\n",
    "In a model without interaction (additive model), each advertising budget contributes to sales independently of the other. The additive model can be represented as:\n",
    "\n",
    "    Sales = beta0 + betatv · betaonline·Online\n",
    "    \n",
    "This equation assumes that the effect of TV and Online advertising on sales are independent, meaning the sales increase from the TV budget is constant regardless of the online ad budget, and vice versa. Each budget’s effect on sales is simply added.\n",
    "\n",
    "In contrast, a model with interaction includes an additional term to account for synergy between the two budgets. The interaction model can be represented as:\n",
    "\n",
    "    Sales = beta0 + betatv·TV + betaOnline·Online + \n",
    "    betatv,online ·(TV * Online)\n",
    "\n",
    "Here, the term betatv,online·(TV * Online) captures the interaction effect, allowing for the possibility that each advertising type’s effect on sales depends on the level of the other. For example, if the online budget is high, the effect of TV ads on sales might increase more than it would independently.\n",
    "\n",
    "How to Use the Models for Predictions:\n",
    "To use the additive model for predictions, plug in the values for TV and Online spending into the equation. The additive model assumes each dollar spent on either channel has an independent, constant effect on sales. To use the interaction model, include the interaction term when plugging in TV and Online spending values. This model predicts that spending on one channel (e.g., TV) could have a greater effect on sales if the other channel’s spending (e.g., Online) is also high.\n",
    "\n",
    "General Difference:\n",
    "The additive model provides a simpler, independent view of each ad type’s contribution to sales, while the interaction model provides a more complex view, capturing synergy where high spending on both channels results in more than just the sum of their parts.\n",
    "\n",
    "\n",
    "\n",
    "Binary Predictor Models for High/Low Ad Budgets\n",
    "\n",
    "If the advertising budgets are categorized as either \"High\" or \"Low\", we use indicator variables for each type: 1 if high and 0 if low. This gives us two models again, one without interaction and one with interaction.\n",
    "\n",
    "The additive model without interaction is given by:\n",
    "    \n",
    "    Sales = beta0 + betatv·1(TV) + betaonline·1(Online)\n",
    "    \n",
    "This model assumes that if either ad budget is high, sales will increase by a fixed amount based on that ad type, with no additional effect from both being high.\n",
    "\n",
    "The interaction model with binary predictors is given by:\n",
    "\n",
    "    Sales = beta0 + betatv·1(TV) + betaonline·1(Online) + \n",
    "    betatv,online·(1(TV) * 1(Online))\n",
    "\n",
    "Here, the interaction term betatv,online·(1(TV)*1(Online)) represents a combined effect when both TV and Online budgets are high, allowing for an additional boost in sales when both are set to high.\n",
    "\n",
    "How to Use the Binary Models for Predictions:\n",
    "In the additive binary model, substitute 1 for high and 0 for low to predict sales based on whether each budget is high or low. The effect of having both budgets high is just the sum of their individual effects. In the interaction binary model, if both budgets are high, include the interaction term in the calculation. This model allows for a greater-than-additive increase in sales when both budgets are high.\n",
    "\n",
    "General Difference:\n",
    "In the additive binary model, the total effect of high budgets in both channels is simply the sum of each high budget’s effect. The interaction binary model, however, predicts an extra boost in sales when both budgets are high, capturing a synergistic interaction where the total effect is more than just the sum of the individual high-budget effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6349bd63",
   "metadata": {},
   "source": [
    "Question 4: \n",
    "\n",
    "The first statement, that “the model only explains 17.6% of the variability in the data,” refers to R-squared, which tells us about the overall fit of the model to the data. An R-squared of 0.176 (or 17.6%) means that the model explains just a small portion of the total variability in the outcome. In other words, most of the variability in the outcome is due to factors not captured by this model. Low R-squared values often suggest that the model may be missing important predictors or that the relationship between predictors and the outcome might not be well-described by a simple linear model.\n",
    "\n",
    "The second statement, “many of the coefficients are larger than 10 while having strong or very strong evidence against the null hypothesis of 'no effect,’” is about individual predictor significance. This significance is assessed by p-values for each coefficient, which tell us whether a specific predictor variable has a statistically significant effect on the outcome variable. Low p-values (strong evidence against the null hypothesis) indicate that these predictors are meaningfully associated with the outcome, even if they don’t collectively explain a large portion of the overall variability.\n",
    "\n",
    "Why This Isn’t a Contradiction\n",
    "1. R-squared vs. p-values address different questions:\n",
    "    - R-squared assesses the overall explanatory power of the model. A low R-squared means that the model as a whole doesn’t account for much of the outcome’s variability.\n",
    "    - P-values measure individual predictor significance. Even with a low R-squared, some predictors can still have statistically significant effects on the outcome.\n",
    "    \n",
    "2. Small overall impact, but meaningful individual effects:\n",
    "    - The model might not capture most of the outcome’s variability (hence, the low R-squared), but individual predictors can still have detectable effects. For example, specific factors like \"Sp. Def\" or \"Generation\" could have a statistically significant relationship with HP, meaning they impact HP in a measurable way, even if the combined influence of all predictors doesn’t fully explain HP’s variability.\n",
    "\n",
    "3. Practical Interpretation:\n",
    "    - Imagine a predictor like “Generation” has a strong effect on the outcome, HP, making Pokémon from Generation 2 have a higher average HP than those from Generation 1. The large coefficient (greater than 10) and low p-value show that this effect is statistically significant. However, because HP may also depend on other unmodeled factors (such as unobserved characteristics or random variation), this model still has a low R-squared, capturing only a portion of HP’s variability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379274d",
   "metadata": {},
   "source": [
    "Question 7: \n",
    "\n",
    "From Model3_fit and Model4_fit to Model5_linear_form:\n",
    "Starting with Model 5, an inclusive approach was taken by adding a large number of predictors, including continuous variables (e.g., Attack, Defense, Speed, etc.) and categorical metrics (e.g., Generation, Type, and Legendary Status). This broad range helps establish a baseline for explaining HP variation.\n",
    "\n",
    "From model 5_linear_form to model 6_linear_form:\n",
    "In Model 6, the predictor variables in Model 5 were refined based on their significance, retaining only those with strong predictive power, thereby simplifying the model, eliminating unnecessary noise, and focusing on the most relevant categories, such as specific “generations” and ” Types”. The purpose of this simplification is to improve the generalizability of the model without overfitting.\n",
    "\n",
    "From Model6_linear_form to Model7_linear_form:\n",
    "Finally, in Model 7, interactions between key variables (e.g., Attack, Speed, Special Defense, and Special Attacks) were introduced because I hypothesized that these variables may not be strictly independent, but rather interact in meaningful ways to affect HP.To manage the multicollinearity that arises from these interactions, a centered and scaled version of the model (model7_CS) was created to improve the stability and interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa000a",
   "metadata": {},
   "source": [
    "Question 9: \n",
    "\n",
    "When choosing regression models, it’s important to balance simplicity, interpretability, and generalizability. While more complex models can sometimes look impressive initially, they often bring issues like overfitting, which makes it critical to weigh the pros and cons carefully.\n",
    "\n",
    "For example, model7 initially seemed to perform better out-of-sample than model6, but it turned out to be more complex. This added complexity can be a problem since it increases the risk of overfitting—basically, the model might pick up on patterns unique to the training data that don’t actually apply to new data. On the other hand, simpler models like model6 tend to avoid this trap because they’re less likely to latch onto patterns that aren’t generalizable.\n",
    "\n",
    "One way to check how reliable a model is involves looking at p-values in the summary. Model7 had a lot of higher p-values, meaning weaker statistical evidence, whereas model6 had a simpler structure with more significant and stable relationships. This stability adds to model6’s strength as it provides more reliable relationships we can interpret with confidence.\n",
    "\n",
    "Interpretability also matters a lot. Because model6 is simpler, it’s easier to understand and explain—fewer variables and interactions are involved. Model7, however, had a complicated four-way interaction among “Attack,” “Speed,” “Sp. Def,” and “Sp. Atk,” which makes it harder to interpret. This complexity reduces its usefulness if the goal is to gain insights or understand how the variables actually interact.\n",
    "\n",
    "Testing these models practically underscored the importance of generalizability. Real-world data, like Pokémon generations, changes over time. A model that performs well on older data might not be as effective for predicting future data. When we used model6 and model7 on data from different generations, model7’s complexity made it less effective at predicting newer data, while model6’s simplicity allowed it to generalize better. This highlights how a simpler model can often outperform a complex one when it comes to handling unseen data.\n",
    "\n",
    "Overall, if a simpler model performs comparably to a more complex one, it’s usually the better choice because it’s more stable, easier to interpret, and often better at generalizing to new data. A complex model only makes sense if it clearly outperforms the simpler one in both in-sample and out-of-sample predictions without signs of overfitting. In this case, although model7 initially looked promising, model6’s interpretability and consistent performance make it the preferred option. This example shows why, in real-world applications, it’s wise to favor simpler models, as they’re often more reliable and easier to understand, especially when similar performance is achievable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4529f8f",
   "metadata": {},
   "source": [
    "Chatbot Interaction Summary: https://chatgpt.com/share/67345f17-5aec-8002-bbe5-6173d3381064"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
